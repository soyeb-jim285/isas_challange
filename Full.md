
# Abstract
Current care centers often lack sophisticated equipment for continuous monitoring of individuals with developmental disabilities, creating risks of unnoticed behavioral changes. This paper presents an enhanced pose-based Long Short-Term Memory (LSTM) model for abnormal activity detection in caregiving scenarios. We extract 18 comprehensive pose features including motion dynamics (hand/foot speeds and accelerations), spatial relationships (joint angles and limb spans), and displacement patterns (head, body center, and eye movements) from skeleton keypoints, with temporal smoothing applied to 30 fps YOLOv7 pose estimation data. Through Leave-One-Subject-Out (LOSO) cross-validation on the ISAS 2025 dataset , we compare baseline and optimized LSTM models across 8 activities—4 normal (walking, sitting, using phone, eating) and 4 abnormal (attacking, head banging, throwing things, biting nails). The optimized model, with temporal parameter tuning (window size=90 frames, LSTM units=64), shows mixed but promising results: significant improvement for 'throwing things' (+14.33 percentage points, from 55.35% to 69.68% F1-score), modest gains for 'head banging' (+0.96 percentage points), but decreased performance for 'biting nails' (-2.55 percentage points). Overall performance demonstrates modest improvements with accuracy increasing from 58.79% to 59.69% (+0.9 percentage points) and weighted F1-score from 58.02% to 58.76% (+0.74 percentage points). While the macro F1-score improvement (+1.51 percentage points) and enhanced consistency across participants (reduced standard deviation) are encouraging, the overall performance levels indicate substantial room for improvement before clinical deployment. The enhanced architecture with bidirectional LSTM layers, dropout regularization, and batch normalization shows better sensitivity to abnormal behaviors compared to baseline approaches, contributing foundational insights for automated behavioral monitoring systems in developmental disability care settings.

# Introduction

The identification of abnormal behavior in individuals with developmental disabilities is a serious issue in care institutions. Actions such as object throwing, attacking, head banging, or nail biting are generally irregular, unplanned, and difficult to recognize due to their subtle manifestations and infrequent occurrence. These actions are very risky to the safety and health of individuals and must be resolved immediately. People with intellectual disabilities face unique healthcare challenges that require specialized attention and improved care coordination [4]. However, staff shortages in care institutions make this issue even more challenging, with less opportunity for round-the-clock human surveillance [1], [2]. Standard detection methods for abnormal behaviors fail to leverage the variability and complexity of such behavior and therefore frequently miss or incorrectly classify it. Elimination of this is not only important for protection of individuals with developmental disabilities but also for maximizing the effectiveness of caregiving in resource-poor environments.

Human pose estimation (HPE), a computer vision technique for finding body key points in images or video, is a feasible solution to abnormal activity recognition [5, 6]. Being able to track postures and movements and identify minute deviations of abnormal behaviors, HPE can be utilized for real-time tracking without the need for intrusive sensors [7]. This technology is particularly valuable in care homes, where it can serve as an adjunct to human care by providing continuous monitoring and alerting staff to potential issues [8]. With individuals who have developmental disabilities, HPE can identify faint patterns of movement, enabling earlier intervention and better care outcomes. Furthermore, the capability of HPE to work with regular cameras, including smartphone cameras, is cheap and scalable for understaffed care homes [9].

With recent developments in artificial intelligence, particularly Large Language Models (LLMs), synthetic data generation techniques have emerged to address data scarcity challenges [8]. Recent comprehensive reviews [9] have highlighted the growing potential of LLMs in activity recognition applications. To address data scarcity in abnormal behavior recognition, researchers have leveraged LLMs to generate synthetic training samples for rare abnormal activities [8]. This approach has demonstrated significant improvement in recognizing complicated behaviors, with high precision for recognizing actions such as throwing objects or self-injurious behavior. The combination of natural language processing and computer vision has significant potential for revolutionizing activity recognition in care settings, where timely and accurate identification is important.

This paper introduces a novel abnormal activity recognition method with pose estimation and LLMs tailored for developmental disability individuals. A novel dataset has been formed based on annotated abnormal behaviors collected from simulated activities over four days involving throwing objects, attacking, head banging, and nail biting, following established ethical protocols. Unlike general-purpose datasets such as NTU RGB+D [22] which focuses on 120 everyday actions, our dataset specifically targets care-relevant abnormal behaviors that pose safety risks in developmental disability care settings. Our method is based on YOLOv7 pose data and LLMs to optimize temporal parameters, such as window size, overlap ratio, and LSTM sequence length, to enhance recognition accuracy. Experimental results indicate the effectiveness of our approach. The baseline experiments attained an accuracy of 0.5041, an F1 weighted score of 0.4971, and an F1 macro of 0.4842. In contrast, our optimized method attained a 0.5354 accuracy, a 0.5371 F1 weighted score, and a 0.5309 F1 macro score.

The implications of this study go beyond technical achievement. By automating abnormal behavior detection, our method tackles staffing gaps for care facilities, allowing caregivers to concentrate on demanding tasks while maintaining round-the-clock monitoring [2]. Furthermore, the integration of HPE and LLMs opens up new possibilities for tailored care by allowing tailored interventions based on individual movement activities and behavior [3]. This technology can improve safety, enhance the quality of care, and guarantee the well-being of people with developmental disabilities.

  This paper provides an in-depth explanation of our process, including building a new dataset, integrating LLMs with pose estimation, and rigorous testing of our approach against traditional methods. We also talk about broader implications of this technology for nursing homes, including its role in addressing staffing challenges as well as improved developmentally disabled outcomes. Through this work, we intend to contribute toward advanced automated monitoring systems to provide safer and more efficient care environments.

# Literature

Skeleton-based human activity recognition (HAR) is of critical relevance in healthcare, surveillance, and human-computer interaction [11]. Effective recognition heavily relies on feature extraction techniques, model structures, and evaluation techniques. Feature extraction also plays a crucial role in HAR performance, with handcrafted features like joint locations, velocities, and angles providing interpretability as well as invariance to changes like body size and camera views [10],[12]. These handcrafted features well represent structured activities like walking but may not well represent complex abnormal movements like falls, where abnormal joint configurations prevail [11]. Deep learning model learned features, however, are able to automatically mine rich spatio-temporal patterns needed for abnormal activities, though at the cost of increased dataset and computation [13]. Consequently, learned vs. hand-designed feature selection is a function of application complexity and resources available [12].

From a modeling architecture perspective, Long Short-Term Memory (LSTM) networks have been widely used due to their capability in learning temporal dependencies in sequential skeleton data, showing drastic accuracy improvements when incorporated with Convolutional Neural Networks (CNNs) [14], [15]. Yet, their shortcoming in representing spatial joint relations resulted in the utilization of Graph Convolutional Networks (GCNs) that have skeleton data as nodes and edges representing their relations and delivered impressive performance improvements for complex and anomalous activities [12]. The following advances are Temporal Convolutional Networks (TCNs) and transformer-like networks, which both effectively capture the short- and long-range dependences in activities [16], [17]. Attention-based hybrid models such as Graph Convolutional Attention-LSTM (GCA-LSTM) utilize attention to concentrate on informative joints, enhancing performance but for which evidence of benefit over standard LSTMs relies on datasets and activity complexity [12]–[14].

Validation methods also critically impact HAR model performance and generalizability. While computationally cheap, K-fold cross-validation (k-CV) risks overstating performance due to subject correlation in training and test sets, exaggerating accuracy [18], [20]. Leave-One-Subject-Out (LOSO) cross-validation, although computationally costly, provides realistic estimates of performance by testing on entirely new subjects with significantly less bias and more generalizability towards abnormal activity recognition with high inter-subject variability [19], [20]. Although the high computational cost and need for heterogeneous datasets have limited LOSO's general application in spite of clear advantages towards feature generalization and model robustness [20].

In fall detection and other abnormal activity detection applications, there are challenges specific to HAR like class imbalance and noisy keypoint detections [21]. Oversampling, undersampling, and weighted loss functions are traditional methods that mitigate class imbalance and boost detection recall for infrequent events. Recent approaches include data augmentation techniques [23] and synthetic data generation [8] to address these challenges. Attention mechanisms improve accuracy by selectively concentrating on salient joints, improving resistance to noisy detections [12].Multi-modal methods that combine skeleton data with RGB or depth images enhance robustness but require additional computational resources. The trade-off between computational tractability and recognition accuracy is still significant.

Briefly, significant trends in skeleton-based abnormal activity recognition are leveraging deep model architectures such as GCNs and hybrid models to overcome spatial modeling weakness inherent in LSTMs, employing strict validation methods such as LOSO to ensure generalizability, and overcoming actual-world challenges such as class imbalance and data noise using optimal feature extraction and attention mechanisms. In response to these gaps recognized, recent studies seek to build sound, generalizable, and computationally light HAR pipelines, essential for realistic real-world implementation [12]–[14], [20], [22].

# References

[1] M. Buntinx and A. Schalock, "Models of disability, quality of life, and individualized supports: Implications for professional practice in intellectual disability," Journal of Policy and Practice in Intellectual Disabilities, vol. 7, no. 4, pp. 283-294, 2010, doi: 10.1111/j.1741-1130.2010.00278.x.

[2] E. Emerson and C. Hatton, "Health inequalities and people with intellectual disabilities," Cambridge University Press, 2014.

[3] A. Bulling, U. Blanke, and B. Schiele, "A tutorial on human activity recognition using body-worn inertial sensors," ACM Computing Surveys, vol. 46, no. 3, pp. 1-33, 2014, doi: 10.1145/2499621.

[4] N. Ashok, D. Hughes, and S. Yardley, "Challenges and opportunities for improvement when people with intellectual disabilities or serious mental illness also need palliative care: A qualitative meta-ethnography," Palliat. Med., vol. 37, no. 8, pp. 1047–1062, Sep. 2023, doi: 10.1177/02692163231175928.

[5] S.-R. Ke, H. U. T. Le, J.-H. Yoo, Y.-J. Lee, J.-N. Hwang, and K.-H. Choi, "A review on video-based human activity recognition," Computers, vol. 2, no. 2, pp. 89–93, 2013, doi: 10.3390/computers2020089.

[6] S. Inoue, P. Lago, T. Hossain, T. Mairittha, and N. Mairittha, "Integrating activity recognition and nursing care records: The system, deployment, and a verification study," Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., vol. 3, no. 3, Sep. 2019, doi: 10.1145/3351244.

[7] R. Morais, V. Le, T. Tran, B. Saha, M. Mansour, and S. Venkatesh, "Learning regularity in skeleton trajectories for anomaly detection in videos," in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Long Beach, CA, USA, Jun. 2019, pp. 11996–12004, doi: 10.1109/CVPR.2019.01227.

[8] J. Wang, Y. Chen, S. Hao, X. Peng, and L. Hu, "Deep learning for sensor-based activity recognition: A survey," Pattern Recognition Letters, vol. 119, pp. 3-11, 2019, doi: 10.1016/j.patrec.2018.02.010.

[9] F. J. Ordóñez and D. Roggen, "Deep convolutional and LSTM recurrent neural networks for multimodal wearable activity recognition," Sensors, vol. 16, no. 1, p. 115, 2016, doi: 10.3390/s16010115.

[10] H. Ramirez, S.A. Velastin, P. Aguayo, E. Fabregas, and G. Farias, "Human activity recognition by sequences of skeleton features," Sensors, vol. 22, no. 11, p. 3991, Jun. 2022, doi: 10.3390/s22113991.

[11] J. Liu, A. Shahroudy, D. Xu, A. C. Kot, and G. Wang, "Skeleton-based action recognition using spatio-temporal LSTM network with trust gates," IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, no. 12, pp. 3007–3021, Dec. 2018, doi: 10.1109/TPAMI.2017.2771306.

[12] S. Yan et al., "Spatial temporal graph convolutional networks for skeleton-based action recognition," in Proc. AAAI Conf. Artif. Intell., vol. 32, no. 1, New Orleans, LA, USA, Feb. 2018, pp. 7444–7452, doi: 10.1609/aaai.v32i1.12328.

[13] C. Li et al., "Skeleton-based action recognition using LSTM and CNN," arXiv:1707.02356, Jul. 2017. [Online]. Available: https://arxiv.org/abs/1707.02356

[14] I. Lee et al., "Ensemble deep learning for skeleton-based action recognition using temporal sliding LSTM networks," in Proc. IEEE Int. Conf. Comput. Vis., Venice, Italy, Oct. 2017, pp. 1022–1030, doi: 10.1109/ICCV.2017.116.

[15] Z. Li et al., "Temporal convolutional networks for skeleton-based human action recognition," in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Long Beach, CA, USA, Jun. 2019, pp. 10320–10329, doi: 10.1109/CVPR.2019.01057.

[16] C. Plizzari et al., "Skeleton-based action recognition with spatio-temporal graph convolutional networks," arXiv:2003.00086, Mar. 2020. [Online]. Available: https://arxiv.org/abs/2003.00086

[17] N. Y. Hammerla et al., "Let's (not) stick together: Pairwise similarity biases cross-validation in activity recognition," in Proc. ACM Int. Joint Conf. Pervasive Ubiquitous Comput., Seattle, WA, USA, Sep. 2015, pp. 1041–1051, doi: 10.1145/2750858.2807551.

[18] D. Gholamiangonabadi, N. Kiselov, and K. Grolinger, "Deep neural networks for human activity recognition with wearable sensors: Leave-one-subject-out cross-validation for model selection," IEEE Trans. Biomed. Eng., vol. 67, no. 7, pp. 1959–1969, Jul. 2020, doi: 10.1109/TBME.2019.2955168.

[19] H. Bragança, J.G. Colonna, H.A.B.F. Oliveira, and E. Souto, "How validation methodology influences human activity recognition mobile systems," Sensors, vol. 22, no. 6, p. 2360, Mar. 2022, doi: 10.3390/s22062360.

[20] B. Kwolek et al., "Human fall detection on embedded platform using low-power accelerometer," Sensors, vol. 14, no. 2, pp. 2760–2783, Feb. 2014, doi: 10.3390/s140202760.

[21] K. Chen, D. Zhang, L. Yao, B. Guo, Z. Yu, and Y. Liu, "Deep learning for sensor-based human activity recognition: Overview, challenges, and opportunities," ACM Comput. Surv., vol. 54, no. 4, pp. 1-40, 2021, doi: 10.1145/3447744.

[22] A. Shahroudy et al., "NTU RGB+D: A large scale dataset for 3D human activity analysis," in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., Las Vegas, NV, USA, Jun. 2016, pp. 1010–1019, doi: 10.1109/CVPR.2016.115.

[23] T. Chen, D. Zhou, J. Wang, S. Wang, Y. Guan, X. He, and E. Ding, "Learning multi-granular spatio-temporal graph network for skeleton-based action recognition," in Proceedings of the 29th ACM International Conference on Multimedia, pp. 4334-4342, 2021, doi: 10.1145/3474085.3475574.