
In this study, we set out to close a critical gap in caregiving technology by enhancing LSTM‑based pose‑driven models for abnormal activity recognition in individuals with developmental disabilities; our principal contributions include an 18‑feature skeletal descriptor, a three‑stage temporal parameter grid search, and an architecture that integrates bidirectional LSTMs, dropout, and batch normalization to foster generalization. Empirically, the optimized model lifted weighted F1‑score and accuracy by 1.3 % and 1.5 %, respectively, cut LOSO performance variance by roughly one‑third, and achieved a 14.3 % surge in “Throwing things” detection while preserving near‑perfect recognition of “Walking,” thereby demonstrating that a 90‑frame temporal window captures the multi‑phase dynamics of rare but hazardous behaviors without sacrificing routine‑activity fidelity. These gains translate directly to real‑world impact: automated, privacy‑preserving video analytics can reduce staff workload, raise situational awareness, and expedite interventions in understaffed facilities, ultimately improving safety and quality of life for vulnerable residents. Nonetheless, challenges remain—most notably the persistent confusion among subtle sedentary behaviors, the small, laboratory‑sourced cohort, and the need for real‑time inference on edge devices. Future work should therefore explore multi‑modal fusion (e.g., object context, audio cues), attention mechanisms for adaptive temporal focus, and longitudinal data collection in authentic settings. In conclusion, our findings validate temporal optimization as a practical lever for boosting abnormal‑behavior recognition and lay the groundwork for deploying robust, scalable monitoring systems; next steps include piloting the model in partner care homes, iteratively refining it with in‑the‑wild data, and conducting user‑centered evaluations to balance accuracy with caregiver trust and resident dignity.